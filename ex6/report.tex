\documentclass[a4paper, 12pt]{article}
\title{TMA4280 -- Exercise 6}
\author{Didrik Jonassen \and Imre Kerr}

\begin{document}
\maketitle

\section{Introduction}
    \subsection{The Problem}
    In this project we are going to solve the two-dimensional Poisson problem using both MPI and OpenMP in a hybrid model on the supercomputer Kongull. The two-dimensional Poisson problem is given as:
    $$-\nabla^2u = f \;\;\; in \; \Omega = (0,1) \times (0,1)$$
    $$u = 0 \;\;\; on \; \delta\Omega\:.$$
    To solve this on a computer we will discretize the problem on a finite difference grid with $(n+1)$ points in each spatial direction. The mesh spacing is then $h = \frac{1}{n}$. The standard five-point stencil is used to discretize the Laplace operator.\\
    To solve the system of algebraic equations we apply the Discrete Sine Transform. This lets us solve the problem in $O(n^2log\textit{n})$.\\
    We are given a serial implementation of the problem, and our task is to parallelize it to run on several nodes on a supercomputer. Things to consider while doing this is to make sure that the load is balanced between the different cores. Another important aspect is that the memory access is nonuniform, meaning that we have to take care of where the data is located and how the processors is communicating with each other to get the data that is needed. As part of the algorithm is a matrix transpose there will be a lot of communication between the different cores.
    
\section{Implementation}
    \subsection{Parallelization Strategy}
    Since the problem is solved by taking the discrete sine transform of each column of matrix $G$, it makes sense to distribute the matrix so that each process has $\frac{n}{p}$ columns. Distributing the matrix like this means that we need to update the transpose function.
    
    \subsection{Distributed Transpose}
    Each process has a vertical segment of the matrix. After transposition this segment needs to lie horizontally. Thus the inter-process communication will consist of partitioning this segment into tiles, and sending one tile to each process. This is done by defining a tile datatype with \texttt{MPI\_Type\_vector}, packing these into contiguous memory with \texttt{MPI\_Pack}, sending with \texttt{MPI\_Alltoall}, and unpacking into tiles again with \texttt{MPI\_Unpack}. Note that we use \texttt{MPI\_Alltoall} rather than \texttt{MPI\_Alltoallv}. This simplifies implementation, but requires us to pad the matrix size to a multiple of $p$.
    
    After the all-to-all, each tile is in the right place, but the tiles themselves still need to be transposed. This is done on a single thread.
    
    \subsection{Shared Memory Parallelization}
    There is also some parallelism to be had when doing the discrete sine transform. Since each column is transformed individually, parallelization should be as simple as adding \texttt{\#pragma omp parallel for} in front of the loops. However, the buffer \texttt{z} needs to be private to each thread. So we allocate \texttt{omp\_get\_max\_threads()} of them, and make sure that each call to \texttt{fst\_} and \texttt{fstinv\_} uses the right one.
    
\section{Testing}
    \subsection{Correctness}
    
    \subsection{Speedup}
    To establish a baseline, we first timed the sequential version of the program.
    
    \begin{tabular}{|r|l|}
    \hline
    n & Time \\
    \hline
    1024 & 2.269s \\
    \hline
    2048 & 9.389s \\
    \hline
    4096 & 41.618s \\
    \hline
    8192 & 178.997s \\
    \hline
    16384 & 767.524\\
    \hline
    \end{tabular}

        \subsubsection{Distributed Memory Parallelization}
        These tests were run on three nodes, with 12 processes on each node.

        \begin{tabular}{|r|l|l|}
        \hline
        n & Time & speedup \\
        \hline
        1024 & 0.143s & 15.86 \\
        \hline
        2048 & 0.466s & 20.15 \\
        \hline
        4096 & 1.862s & 22.35 \\
        \hline
        8192 & 7.703s & 23.24\\
        \hline
        16384 & 32.203s & 23.83 \\
        \hline
        \end{tabular}
         
        \subsubsection{Shared Memory Parallelization}
        These tests were run on three nodes, with 2 processes on each node, and 6 threads per process.

        \begin{tabular}{|r|l|l|}
        \hline
        n & Time & speedup \\
        \hline
        1024 & 0.142s & 15.98 \\
        \hline
        2048 & 0.497s & 18.89 \\
        \hline
        4096 & 2.057s & 20.23 \\
        \hline
        8192 & 8.503s & 21.05\\
        \hline
        16384 & 36.762s & 20.88 \\
        \hline
        \end{tabular}
        
        We see that using fewer processes and more threads is slightly slower. We think this could be solved with a parallel tile-transpose. However this would need to be programmed carefully, to properly utilize the cache.
        
    
\end{document}