\documentclass[a4paper, 12pt]{article}
\title{TMA4280 -- Exercise 4}
\author{Didrik Jonassen \and Imre Kerr}

\begin{document}
\maketitle

\section{Description of Solution} % (fold)
\label{sec:description_of_solution}
    \subsection{Serial Version} % (fold)
        The code is organized in a main program and one function, \texttt{S\_N}, which does the actual computation. The main program calls \texttt{S\_N} with various values of \texttt{N}, and writes the result to standard output. The difference $S-S_n$ is written with 15 decimal digits, which roughly corresponds to the precision of double-precision floating point numbers.

        The function \texttt{S\_N} comprises two loops. The first loop generates the vector elements, $v(i) = \frac{1}{i^2}$, using the code \texttt{V(I) = 1.0 / I**2}. (Generated using Fortran's Formula Translator feature.) The second loop computes the sum of these elements by looping through the vector and adding them up. It is worth noting that it sums the numbers from smallest to largest, to minimize loss of precision.
        % subsection serial_version (end)

    \subsection{Shared Memory Version} % (fold)
        The shared memory parallel version of the program is almost identical to the serial version, except for two \texttt{!\$OMP PARALLEL DO} directives. The second of these is a reduction, with the return value \texttt{S\_N} as the reduction variable.
    % subsection shared_memory_version (end)

    \subsection{Distributed Memory Version} % (fold)
        Like the serial and shared memory version, the distributed memory program is also split into a main program and a compute function. The main program contains the setup calls to \texttt{MPI\_INIT}, \texttt{MPI\_COMM\_SIZE} AND \texttt{MPI\_COMM\_RANK}, and the call to \texttt{MPI\_FINALIZE} at the end. Only the process with rank 0 writes any output.

        In the compute function, the rank-0 process is responsible for generating the vector $\underline{v}$ and distributing it to the other processes. When doing this, it will generate one ``chunk'' at a time, and send it to the appropriate process with a call to \texttt{MPI\_SEND}. The process on the receiving end will recieve the chunk with a call to \texttt{MPI\_RECV}. Process 0's own chunk is generated last, this time without a send/receive pair. Chunks are generated in an interleaved fashion, with $\frac{1}{1^2}$ going in the first chunk, $\frac{1}{2^2}$ going in the second, etc. This results in the local sums being similar in magnitude, which again limits the loss of precision when they are added up at the end.

        While doing this, process 0 will re-use the same buffer, thereby ensuring that it never uses more memory than necessary. An alternative method would have been to generate the entire vector and distribute it using \texttt{MPI\_SCATTER}. And while this would certainly be simpler to program, the ``one chunk at a time'' method uses a factor of $P$ less memory, where $P$ is the number of processors.

        After each process recieves its chunk of the vector, it computes a local sum using the same loop as before. Finally, the local sums are added up using a call to \texttt{MPI\_REDUCE}. Process 0 will then return the final sum, while all other processes return an unused value, 0.
    % subsection distributed_memory_version (end)

    \subsection{MPI/OpenMP Version} % (fold)
        Again, this version is only distinguished from the basic MPI version by the two \texttt{!\$OMP PARALLEL DO} directives.
    % subsection mpi_openmp (end)
% section description_of_solution (end)

\section{Results} % (fold)
\label{sec:results}
    Notes about the following tests: 
    \begin{itemize}
        \item For the numerical tests, we give an abbreviated output containing only the differences, $S - S_n$.
        \item For the timing tests, we use the Unix \texttt{time} command to time a single computation of $S_n$, for $n = 2^{27}$. This is done because the problem size given in the exercise, $n = 2^k, k = 3, \ldots, 14$, gives runtimes so short it's impossible to compare them.
        \item All tests were run on a dual core machine with HyperThreading, so the timing tests were done with 4 total threads in each case. (Except for the serial program, of course.)
    \end{itemize}

    \subsection{Serial Version} % (fold)
        \subsubsection{Numerical Results} % (fold)
            \begin{verbatim}
S - SN FOR N = 2^03: 1.17512014971443E-01
S - SN FOR N = 2^04: 6.05875340265711E-02
S - SN FOR N = 2^05: 3.07668046302652E-02
S - SN FOR N = 2^06: 1.55035659995419E-02
S - SN FOR N = 2^07: 7.78206247968138E-03
S - SN FOR N = 2^08: 3.89863112331024E-03
S - SN FOR N = 2^09: 1.95121947262034E-03
S - SN FOR N = 2^10: 9.76086397362463E-04
S - SN FOR N = 2^11: 4.88162639631140E-04
S - SN FOR N = 2^12: 2.44111404580805E-04
S - SN FOR N = 2^13: 1.22063439612630E-04
S - SN FOR N = 2^14: 6.10338704929436E-05
            \end{verbatim}
        % subsection numerical_results (end)
        \subsubsection{Timing} % (fold)
\begin{verbatim}
real    0m1.855s
user    0m1.266s
sys     0m0.581s
\end{verbatim}        
        % subsection timing (end)
    % subsection serial_version (end)

    \subsection{Shared Memory Version} % (fold)
        \subsubsection{Numerical Results} % (fold)
        \begin{verbatim}
S - SN FOR N = 2^03: 1.17512014971443E-01
S - SN FOR N = 2^04: 6.05875340265711E-02
S - SN FOR N = 2^05: 3.07668046302652E-02
S - SN FOR N = 2^06: 1.55035659995419E-02
S - SN FOR N = 2^07: 7.78206247968138E-03
S - SN FOR N = 2^08: 3.89863112331024E-03
S - SN FOR N = 2^09: 1.95121947262034E-03
S - SN FOR N = 2^10: 9.76086397362463E-04
S - SN FOR N = 2^11: 4.88162639631140E-04
S - SN FOR N = 2^12: 2.44111404580805E-04
S - SN FOR N = 2^13: 1.22063439612630E-04
S - SN FOR N = 2^14: 6.10338704929436E-05
        \end{verbatim}     
        % subsection numerical_results (end)
        \subsubsection{Timing} % (fold)
\begin{verbatim}
real    0m0.936s
user    0m2.672s
sys     0m0.740
\end{verbatim}        
        % subsection timing (end)

    % subsection shared_memory_version (end)

    \subsection{Distributed Memory Version} % (fold)
        \subsubsection{Numerical Results, 2 Processes} % (fold)
        \begin{verbatim}
S - SN FOR N = 2^03: 1.17512014971443E-01
S - SN FOR N = 2^04: 6.05875340265711E-02
S - SN FOR N = 2^05: 3.07668046302652E-02
S - SN FOR N = 2^06: 1.55035659995419E-02
S - SN FOR N = 2^07: 7.78206247968138E-03
S - SN FOR N = 2^08: 3.89863112331024E-03
S - SN FOR N = 2^09: 1.95121947262034E-03
S - SN FOR N = 2^10: 9.76086397362463E-04
S - SN FOR N = 2^11: 4.88162639631140E-04
S - SN FOR N = 2^12: 2.44111404580805E-04
S - SN FOR N = 2^13: 1.22063439612630E-04
S - SN FOR N = 2^14: 6.10338704929436E-05
        \end{verbatim}
        % subsection numerical_results_2_processors (end)

        \subsubsection{Numerical Results, 8 Processes} % (fold)
        \begin{verbatim}
S - SN FOR N = 2^03: 1.17512014971443E-01
S - SN FOR N = 2^04: 6.05875340265711E-02
S - SN FOR N = 2^05: 3.07668046302652E-02
S - SN FOR N = 2^06: 1.55035659995419E-02
S - SN FOR N = 2^07: 7.78206247968138E-03
S - SN FOR N = 2^08: 3.89863112331024E-03
S - SN FOR N = 2^09: 1.95121947262034E-03
S - SN FOR N = 2^10: 9.76086397362463E-04
S - SN FOR N = 2^11: 4.88162639631140E-04
S - SN FOR N = 2^12: 2.44111404580805E-04
S - SN FOR N = 2^13: 1.22063439612630E-04
S - SN FOR N = 2^14: 6.10338704929436E-05
        \end{verbatim}
        % subsection numerical_results_8_processes (end)
        \subsubsection{Timing} % (fold)
\begin{verbatim}
real    0m1.952s
user    0m6.586s
sys     0m0.868s
\end{verbatim}        
        % subsection timing (end)
    % subsection distributed_memory_version (end)

    \subsection{MPI/OpenMP Version} % (fold)
        \subsubsection{Numerical Results} % (fold)
        \begin{verbatim}
S - SN FOR N = 2^03: 1.17512014971443E-01
S - SN FOR N = 2^04: 6.05875340265711E-02
S - SN FOR N = 2^05: 3.07668046302652E-02
S - SN FOR N = 2^06: 1.55035659995419E-02
S - SN FOR N = 2^07: 7.78206247968138E-03
S - SN FOR N = 2^08: 3.89863112331024E-03
S - SN FOR N = 2^09: 1.95121947262034E-03
S - SN FOR N = 2^10: 9.76086397362463E-04
S - SN FOR N = 2^11: 4.88162639631140E-04
S - SN FOR N = 2^12: 2.44111404580805E-04
S - SN FOR N = 2^13: 1.22063439612630E-04
S - SN FOR N = 2^14: 6.10338704929436E-05
        \end{verbatim}
        % subsection numerical_results (end)
        \subsubsection{Timing} % (fold)
\begin{verbatim}
real    0m4.134s
user    0m15.395s
sys     0m0.614s
\end{verbatim}        
        % subsection timing (end)
    % subsection mpi_openmp_version (end)

% section results (end)
\section{Discussion} % (fold)
\label{sec:discussion}
    \subsection{Performance}
    In our testing, we saw that only the shared memory version gives any performance benefit. We think this has less to do with the memory model used, and more to do with the fact that having to create the vector on only one process is a bottleneck. One could argue that this means the distributed memory version is not load-balanced. This problem would diminish if the computation was more complex than summing a vector.

    \subsection{FLOPs}
    To generate the vector $\underline{v}$ we only need $n$ FLOPs. The square $i^2$ can be calculated using integer operations, leaving only the division to be calculated using a floating point operation. Equally the calculation of $S_n$ only need $n$ FLOPs, one for each of number in the vector. These numbers does not increase when solving the problem in parallel.

        \subsubsection{Commutativity}
        The fact that floating point operations are not commutative have some implications for the program. First of all it is important to sum the numbers from the smallest to the largest, not the other way around. If it is done incorrectly the error will stop scaling with the size of the vector at some point because the value to be added is too small to make a change to the value gained from adding the previous values in the vector.

        This also has an impact on the result gained from the parallel versions of the program. As the vector is no longer added in the same order the error cannot be expected to stay the same as in the serial version, even if it happened to be the same in our case.

    \subsection{MPI calls}
    The MPI calls needed to make a program run is \texttt{MPI\_INIT}, \texttt{MPI\_COMM\_SIZE}, \texttt{MPI\_COMM\_RANK}, and \texttt{MPI\_FINALIZE}. For the program to do something useful we also need to send data between the processes. As the problem required that we generated the entire vector on one process the most convenient way to spread the subvectors is the function \texttt{MPI\_SCATTER}. The problem with this strategy is that the entire vector must be kept on the generating processor, and therefore the problem size cannot be scaled up with an increased number of cores. The max limit of the problem size is set by the amount of memory on the generating node.

    Instead we opted to generate one part of the vector at a time and use \texttt{MPI\_SEND} and \texttt{MPI\_RECV} to send it to another process for computation. This way you can work on a much larger dataset without being limited by ram on the generating node. If the memory limit is reached one can simply add more nodes to be able to increase the problem size. For gathering up the results the most convenient way is to use \texttt{MPI\_REDUCE}. This function takes care of the logic that is required to optimally sum the individual processes values together and give it to one process.

    As we can see the total memory usage is equal to the single-processor program even when run on several processors, but the total amount of available memory is greater, making the multi-processor program arguably more memory efficient.
    % section discussion (end)

    \subsection{Conclusion}
    We have created several versions of a program to solve a simple, but computationally intensive problem. While this particular problem has little practical relevance, it models problems one can encounter in real life, and can give insights about how best to solve them.
\end{document}